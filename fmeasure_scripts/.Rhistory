load('Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble.Rdata')
View(df)
unique(df$algorithm)
unique(df$dataset)
load('Documents/school/thesis/res')
load('Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble.Rdata')
tmdf <- df[df$dataset == 'tm']
tmdf <- df[,df$dataset == 'tm']
tmdf <- df[df$dataset == 'tm',]
View(tmdf)
load('Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble-tm.Rdata')
df == tmdf
View(df)
View(tmdf)
quit()
load('Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble.Rdata')
df
load('Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble-without-pint.Rdata')
df
load('Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble.Rdata')
write.csv(df, 'Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble.tsv', sep='\t')
write.table(df, 'Documents/school/thesis/results/Rdata/F-Measure-Bootstrap-Ensemble.tsv', sep='\t')
source('Documents/school/thesis/results/fmeasure_scripts/F-Measure-Func.R')
library(dplyr)
library(tidyr)
library(ggplot2)
library(tidytext)
library(janeaustenr)
library(forcats)
Work.Dir <- "~/Documents/school/thesis/results/"
ref <- "truth"
all.datasets <- c("llc")
all.algorithms <- c("metaneighbor")
debugonce('F.Measure.Func')
for (dataset in all.datasets) {
print(dataset)
data.path <- paste0(Work.Dir,"/cellres/", dataset, "_paper_predictions.tsv")
data <- read.csv(data.path, header=T, sep='\t')
for (alg in all.algorithms) {
print(alg)
C <- data[, ref] #truth labels
K <- data[, alg] #algorithm being tested
N <- nrow(data)
bootstrap.func <- function(x) {
indx <- sample(1:N, size=N, replace=T)
C1 <- C[indx]
K1 <- K[indx]
f <- F.Measure.Func(C1, K1)
return(c(f$F.measure,f$sens, f$prec)) #This returns the F.measure not the vector from the function
#return(F.Measure.Func(C1, K1)$F.measure) #This returns the F.measure not the vector from the function
}}
}
debugonce(F.Measure.Func)
for (dataset in all.datasets) {
print(dataset)
data.path <- paste0(Work.Dir,"/cellres/", dataset, "_paper_predictions.tsv")
data <- read.csv(data.path, header=T, sep='\t')
for (alg in all.algorithms) {
print(alg)
C <- data[, ref] #truth labels
K <- data[, alg] #algorithm being tested
N <- nrow(data)
bootstrap.func <- function(x) {
indx <- sample(1:N, size=N, replace=T)
C1 <- C[indx]
K1 <- K[indx]
f <- F.Measure.Func(C1, K1)
return(c(f$F.measure,f$sens, f$prec)) #This returns the F.measure not the vector from the function
#return(F.Measure.Func(C1, K1)$F.measure) #This returns the F.measure not the vector from the function
}
# Identify F measures
##rslt <- unlist(lapply(1:10000, bootstrap.func))
#lapply should return a list of three-tuple
# do.call() will turn the list of three-tuples into the matrix like the for loop
#TODO try do.call(rbind(lapply(),boots))
rslt <- c()
for (i in 1:10000){
rslt <- rbind(rslt,bootstrap.func())
}}}
View(ni)
View(Fm)
View(Fm)
View(Pr)
View(Re)
all.algorithms <- c("gsea")
all.datasets <- c("peng")
debugonce(F.Measure.Func)
for (dataset in all.datasets) {
print(dataset)
data.path <- paste0(Work.Dir,"/cellres/", dataset, "_paper_predictions.tsv")
data <- read.csv(data.path, header=T, sep='\t')
for (alg in all.algorithms) {
print(alg)
C <- data[, ref] #truth labels
K <- data[, alg] #algorithm being tested
N <- nrow(data)
bootstrap.func <- function(x) {
indx <- sample(1:N, size=N, replace=T)
C1 <- C[indx]
K1 <- K[indx]
f <- F.Measure.Func(C1, K1)
return(c(f$F.measure,f$sens, f$prec)) #This returns the F.measure not the vector from the function
#return(F.Measure.Func(C1, K1)$F.measure) #This returns the F.measure not the vector from the function
}
# Identify F measures
##rslt <- unlist(lapply(1:10000, bootstrap.func))
#lapply should return a list of three-tuple
# do.call() will turn the list of three-tuples into the matrix like the for loop
#TODO try do.call(rbind(lapply(),boots))
rslt <- c()
for (i in 1:10000){
rslt <- rbind(rslt,bootstrap.func())
}}}
View(ni)
View(ni)
library('mclust')
library('mclust')
?adjustedRandIndex
version
filename <- tempfile()
utils::download.file("https://uwoca-my.sharepoint.com/:u:/g/personal/pshoosh_uwo_ca/Ef5PYcWJPh1JrhogARjLHWcBNUmSkBF5mNsLF-RbvTFEvA?e=iqeaGe&download=1",
destfile=filename,
mode="wb",
quiet = F)
getwd()
setwd('Documents/school/thesis/results/fmeasure_scripts/')
ll
list.dirs()
list.dir()
list()
list.files()
source('analysis_scripts/Functions.R')
source('Functions.R')
source('F-Measure-Func.R')
Work.Dir <- "~/Documents/school/thesis/results/"
library(dplyr)
library(tidyr)
library(ggplot2)
library(tidytext)
library(janeaustenr)
library(forcats)
ref <- "truth"
all.datasets <- ("jam")
all.algorithms <- ("CaSTLe")
all.datasets <- c("jam")
all.algorithms <- c("CaSTLe")
f.measure <- low.CI <- high.CI <- sens <- low.CI.sens <- high.CI.sens <- prec <- low.CI.prec <- high.CI.prec <- matrix(0, nrow=length(all.algorithms), ncol=length(all.datasets), dimnames=list(all.algorithms, all.datasets))
df <- c()
dataset <- "jam"
data.path <- paste0(Work.Dir,"/cellres/", dataset, "_seurat_predictions.tsv")
data <- read.csv(data.path, header=T, sep='\t')
C <- data[,ref]
K <- data[,alg]
alg <- "CaSTLe"
K <- data[,alg]
N <- nrow(data)
f <- F.Measure.Func(C, K)
f
if (length(C) != length(K)) {
print("Error. The length of vectors should be the same.")
F.measure <- NA
} else {
# c is the truth labels, k is predictions
n <- length(unique(C))
ni <- unique(C)
m <- length(unique(K))
a <- Pr <- Re <- Fm <- Sp <- matrix(0, nrow=n, ncol=n)
F.measure <- Sensitivity <- Specificity <- Precision <- 0
#TODO mention that changing this to a single loop nearly exactly matches sklearn's classification_report weighted avg
for (i in 1:n) #check each truth label
{
j <- i
a[i,j] <- length(intersect(which(C == ni[i]), which(K == ni[i]))) #true positives
# select values where where the prediction is the same as the truth label
# which(c==n) subsets the cells to only those that're actually a truth label
# which(k==n) subsets the cell to only those that're predicted to be the same truth label
if (a[i,j] == 0) { Fm[i,j]} else {
Pr[i,j] <- a[i,j]/length(which(K == ni[i]))
Re[i,j] <- a[i,j]/length(which(C == ni[i]))
Fm[i,j] <- (2 * Pr[i,j] * Re[i,j])/(Pr[i,j] + Re[i,j])
}
w <- length(which(C == ni[i]))/length(C)
F.measure <- F.measure + w * max(Fm[i,]) # weighted f-measure, w is proportion of total cells that is a particular cells by the truth labels
Sensitivity <- Sensitivity + w * max(Re[i,]) # weighted f-measure, w is proportion of total cells that is a particular cells by the truth labels
Precision <- Precision + w * max(Pr[i,]) # weighted f-measure, w is proportion of total cells that is a particular cells by the truth labels
}
F.vector <- apply(Fm, 1, max) # this is a per-cell type f-measure
}
View(Pr)
View(Re)
View(Fm)
View(sens)
alg <- "cibersort"
data.path <- paste0(Work.Dir,"/cellres/", dataset, "_paper_predictions.tsv")
data <- read.csv(data.path, header=T, sep='\t')
K <- data[,alg]
N <- nrow(data)
C <- data[,ref]
if (length(C) != length(K)) {
print("Error. The length of vectors should be the same.")
F.measure <- NA
} else {
# c is the truth labels, k is predictions
n <- length(unique(C))
ni <- unique(C)
m <- length(unique(K))
a <- Pr <- Re <- Fm <- Sp <- matrix(0, nrow=n, ncol=n)
F.measure <- Sensitivity <- Specificity <- Precision <- 0
#TODO mention that changing this to a single loop nearly exactly matches sklearn's classification_report weighted avg
for (i in 1:n) #check each truth label
{
j <- i
a[i,j] <- length(intersect(which(C == ni[i]), which(K == ni[i]))) #true positives
# select values where where the prediction is the same as the truth label
# which(c==n) subsets the cells to only those that're actually a truth label
# which(k==n) subsets the cell to only those that're predicted to be the same truth label
if (a[i,j] == 0) { Fm[i,j]} else {
Pr[i,j] <- a[i,j]/length(which(K == ni[i]))
Re[i,j] <- a[i,j]/length(which(C == ni[i]))
Fm[i,j] <- (2 * Pr[i,j] * Re[i,j])/(Pr[i,j] + Re[i,j])
}
w <- length(which(C == ni[i]))/length(C)
F.measure <- F.measure + w * max(Fm[i,]) # weighted f-measure, w is proportion of total cells that is a particular cells by the truth labels
Sensitivity <- Sensitivity + w * max(Re[i,]) # weighted f-measure, w is proportion of total cells that is a particular cells by the truth labels
Precision <- Precision + w * max(Pr[i,]) # weighted f-measure, w is proportion of total cells that is a particular cells by the truth labels
}
F.vector <- apply(Fm, 1, max) # this is a per-cell type f-measure
}
